{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yANTp54UjfaW",
        "outputId": "1e86a775-75da-49a3-8eba-9d5235a122d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 668/668 [00:02<00:00, 321.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 668/668 [00:02<00:00, 322.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 0.3909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 668/668 [00:02<00:00, 322.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 0.2720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 668/668 [00:02<00:00, 327.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 0.1795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 668/668 [00:02<00:00, 325.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 0.1060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 668/668 [00:02<00:00, 323.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 0.0657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 668/668 [00:02<00:00, 317.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 0.0431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 668/668 [00:02<00:00, 319.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 0.0314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 668/668 [00:02<00:00, 320.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 0.0316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 668/668 [00:02<00:00, 322.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 0.0219\n",
            "Test Accuracy: 81.45%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.91      0.85      2996\n",
            "         1.0       0.86      0.69      0.77      2346\n",
            "\n",
            "    accuracy                           0.81      5342\n",
            "   macro avg       0.82      0.80      0.81      5342\n",
            "weighted avg       0.82      0.81      0.81      5342\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2731  265]\n",
            " [ 726 1620]]\n",
            "Training Accuracy: 99.72%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "\n",
        "# -------------------------------\n",
        "# Data Loading\n",
        "# -------------------------------\n",
        "df = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)\n",
        "texts = df['headline']\n",
        "labels = df['is_sarcastic']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------------------\n",
        "# Tokenization & Vocabulary Building\n",
        "# -------------------------------\n",
        "def tokenize(text):\n",
        "    # A simple tokenizer that lowercases and splits on whitespace.\n",
        "    return text.lower().split()\n",
        "\n",
        "# Build vocabulary from training data\n",
        "all_tokens = []\n",
        "for text in X_train:\n",
        "    all_tokens.extend(tokenize(text))\n",
        "counter = Counter(all_tokens)\n",
        "# Reserve 0 for <PAD> and 1 for <UNK>\n",
        "vocab = {token: idx+2 for idx, (token, count) in enumerate(counter.most_common())}\n",
        "vocab[\"<PAD>\"] = 0\n",
        "vocab[\"<UNK>\"] = 1\n",
        "\n",
        "# -------------------------------\n",
        "# Convert Texts to Sequences & Padding\n",
        "# -------------------------------\n",
        "def text_to_sequence(text, vocab):\n",
        "    tokens = tokenize(text)\n",
        "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "def pad_sequence(seq, max_length):\n",
        "    if len(seq) < max_length:\n",
        "        return seq + [vocab[\"<PAD>\"]] * (max_length - len(seq))\n",
        "    else:\n",
        "        return seq[:max_length]\n",
        "\n",
        "max_length = 20  # You can adjust this based on typical headline lengths\n",
        "\n",
        "X_train_seq = [pad_sequence(text_to_sequence(text, vocab), max_length) for text in X_train]\n",
        "X_test_seq = [pad_sequence(text_to_sequence(text, vocab), max_length) for text in X_test]\n",
        "\n",
        "# -------------------------------\n",
        "# Convert to Tensors & Create DataLoaders\n",
        "# -------------------------------\n",
        "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------\n",
        "# LSTM Model Definition\n",
        "# -------------------------------\n",
        "class LSTMSarcasmDetector(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
        "        super(LSTMSarcasmDetector, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: [batch_size, max_length]\n",
        "        embedded = self.embedding(text)  # [batch_size, max_length, embedding_dim]\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # Use the last hidden state from the final LSTM layer\n",
        "        hidden_last = hidden[-1]  # [batch_size, hidden_dim]\n",
        "        out = self.fc(hidden_last)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "model = LSTMSarcasmDetector(vocab_size, embedding_dim, hidden_dim, n_layers, dropout)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Loop\n",
        "# -------------------------------\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluation\n",
        "# -------------------------------\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            predictions = (outputs >= 0.5).float()\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(batch_y.cpu().numpy())\n",
        "    return np.array(all_preds).flatten(), np.array(all_labels).flatten()\n",
        "\n",
        "preds, true_labels = evaluate(model, test_loader, device)\n",
        "acc = accuracy_score(true_labels, preds)\n",
        "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels, preds))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, preds))\n",
        "\n",
        "\n",
        "# Evaluate training accuracy\n",
        "train_preds, train_labels = evaluate(model, train_loader, device)\n",
        "train_acc = accuracy_score(train_labels, train_preds)\n",
        "print(f\"Training Accuracy: {train_acc * 100:.2f}%\")"
      ]
    }
  ]
}